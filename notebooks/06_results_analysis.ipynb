{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqymBKIaYm3_",
        "outputId": "83efc747-cdea-4dcf-a179-26e8c1651463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSCI316_Project1_Regression_4_2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AotYQO04ZPpJ",
        "outputId": "7413f31c-e2ed-4717-9a42-a41aa9b2d481"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PARQUET_DIR = \"/content/drive/MyDrive/cleaned_parquet\"\n",
        "\n",
        "import os\n",
        "parquet_files = [f for f in os.listdir(PARQUET_DIR) if f.endswith(\".parquet\")]\n",
        "print(\"Parquet files found:\", len(parquet_files))\n",
        "print(parquet_files[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49zySz_pZPmy",
        "outputId": "f5f35113-e553-42f4-b96c-b006e59e9bdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet files found: 200\n",
            "['part-00172-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00194-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00185-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00188-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00196-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00178-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00169-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00182-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00193-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00159-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00199-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00167-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00195-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00161-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00191-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00155-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00190-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00180-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00157-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet', 'part-00181-1fd552f4-5ba5-48c4-ad91-1cb3b7309ccc-c000.snappy.parquet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(PARQUET_DIR)\n",
        "print(\"Loaded parquet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqaSZxETZPkV",
        "outputId": "c6c58e59-f8db-46ae-e07b-e4d825cef5af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "print(\"\\nRow count:\", df.count())\n",
        "\n",
        "print(\"\\nSample rows:\")\n",
        "df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L30VaAwOa7Gx",
        "outputId": "c4d4fb15-4efd-402e-892f-bb651aa87fe7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema:\n",
            "root\n",
            " |-- transaction_id: string (nullable = true)\n",
            " |-- procedure_id: integer (nullable = true)\n",
            " |-- trans_group_id: integer (nullable = true)\n",
            " |-- trans_group_ar: string (nullable = true)\n",
            " |-- trans_group_en: string (nullable = true)\n",
            " |-- procedure_name_ar: string (nullable = true)\n",
            " |-- procedure_name_en: string (nullable = true)\n",
            " |-- instance_date: string (nullable = true)\n",
            " |-- property_type_id: integer (nullable = true)\n",
            " |-- property_type_ar: string (nullable = true)\n",
            " |-- property_type_en: string (nullable = true)\n",
            " |-- property_sub_type_id: integer (nullable = true)\n",
            " |-- property_sub_type_ar: string (nullable = true)\n",
            " |-- property_sub_type_en: string (nullable = true)\n",
            " |-- property_usage_ar: string (nullable = true)\n",
            " |-- property_usage_en: string (nullable = true)\n",
            " |-- reg_type_id: integer (nullable = true)\n",
            " |-- reg_type_ar: string (nullable = true)\n",
            " |-- reg_type_en: string (nullable = true)\n",
            " |-- area_id: integer (nullable = true)\n",
            " |-- area_name_ar: string (nullable = true)\n",
            " |-- area_name_en: string (nullable = true)\n",
            " |-- building_name_ar: string (nullable = true)\n",
            " |-- building_name_en: string (nullable = true)\n",
            " |-- project_number: string (nullable = true)\n",
            " |-- project_name_ar: string (nullable = true)\n",
            " |-- project_name_en: string (nullable = true)\n",
            " |-- master_project_en: string (nullable = true)\n",
            " |-- master_project_ar: string (nullable = true)\n",
            " |-- nearest_landmark_ar: string (nullable = true)\n",
            " |-- nearest_landmark_en: string (nullable = true)\n",
            " |-- nearest_metro_ar: string (nullable = true)\n",
            " |-- nearest_metro_en: string (nullable = true)\n",
            " |-- nearest_mall_ar: string (nullable = true)\n",
            " |-- nearest_mall_en: string (nullable = true)\n",
            " |-- rooms_ar: string (nullable = true)\n",
            " |-- rooms_en: string (nullable = true)\n",
            " |-- has_parking: integer (nullable = true)\n",
            " |-- procedure_area: double (nullable = true)\n",
            " |-- actual_worth: double (nullable = true)\n",
            " |-- meter_sale_price: double (nullable = true)\n",
            " |-- rent_value: double (nullable = true)\n",
            " |-- meter_rent_price: double (nullable = true)\n",
            " |-- no_of_parties_role_1: integer (nullable = true)\n",
            " |-- no_of_parties_role_2: integer (nullable = true)\n",
            " |-- no_of_parties_role_3: integer (nullable = true)\n",
            " |-- instance_ts: timestamp (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- dow: integer (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\", line 535, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/socket.py\", line 720, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2880168537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRow count:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSample rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Create real date column from the string\n",
        "df2 = df.withColumn(\"instance_date_dt\", F.to_date(F.col(\"instance_date\"), \"dd-MM-yyyy\"))\n",
        "\n",
        "\n",
        "df2.select(\"instance_date\", \"instance_date_dt\").show(10, truncate=False)\n",
        "\n",
        "# Create consistent feature columns (year/month/dow)\n",
        "df2 = (\n",
        "    df2.withColumn(\"year\", F.year(\"instance_date_dt\"))\n",
        "       .withColumn(\"month\", F.month(\"instance_date_dt\"))\n",
        "       .withColumn(\"dow\", F.dayofweek(\"instance_date_dt\"))\n",
        ")\n",
        "\n",
        "df2.select(\"instance_date\", \"instance_date_dt\", \"year\", \"month\", \"dow\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tvGdvHla7DU",
        "outputId": "5158c2b2-36dc-46d4-9709-0228630a910d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------------+\n",
            "|instance_date|instance_date_dt|\n",
            "+-------------+----------------+\n",
            "|05-07-2010   |2010-07-05      |\n",
            "|24-02-2009   |2009-02-24      |\n",
            "|14-08-2007   |2007-08-14      |\n",
            "|04-09-2018   |2018-09-04      |\n",
            "|02-07-2007   |2007-07-02      |\n",
            "|26-10-2009   |2009-10-26      |\n",
            "|25-07-2011   |2011-07-25      |\n",
            "|01-03-2010   |2010-03-01      |\n",
            "|29-03-2010   |2010-03-29      |\n",
            "|13-06-2018   |2018-06-13      |\n",
            "+-------------+----------------+\n",
            "only showing top 10 rows\n",
            "+-------------+----------------+----+-----+---+\n",
            "|instance_date|instance_date_dt|year|month|dow|\n",
            "+-------------+----------------+----+-----+---+\n",
            "|05-07-2010   |2010-07-05      |2010|7    |2  |\n",
            "|24-02-2009   |2009-02-24      |2009|2    |3  |\n",
            "|14-08-2007   |2007-08-14      |2007|8    |3  |\n",
            "|04-09-2018   |2018-09-04      |2018|9    |3  |\n",
            "|02-07-2007   |2007-07-02      |2007|7    |2  |\n",
            "|26-10-2009   |2009-10-26      |2009|10   |2  |\n",
            "|25-07-2011   |2011-07-25      |2011|7    |2  |\n",
            "|01-03-2010   |2010-03-01      |2010|3    |2  |\n",
            "|29-03-2010   |2010-03-29      |2010|3    |2  |\n",
            "|13-06-2018   |2018-06-13      |2018|6    |4  |\n",
            "+-------------+----------------+----+-----+---+\n",
            "only showing top 10 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "label_col = \"meter_sale_price\"\n",
        "numeric_cols = [\"procedure_area\", \"has_parking\", \"year\", \"month\", \"dow\"]\n",
        "cat_cols = [\"area_name_en\", \"nearest_metro_en\", \"nearest_mall_en\", \"nearest_landmark_en\",\n",
        "            \"property_type_en\", \"rooms_en\"]\n",
        "\n",
        "needed_cols = [label_col] + numeric_cols + cat_cols\n",
        "\n",
        "missing_exprs = [F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in needed_cols]\n",
        "missing_row2 = df2.select(missing_exprs).collect()[0].asDict()\n",
        "missing_row2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYz1hJwoNifx",
        "outputId": "a1181310-a34e-47f0-9e92-184feb13f26f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meter_sale_price': 0,\n",
              " 'procedure_area': 0,\n",
              " 'has_parking': 0,\n",
              " 'year': 0,\n",
              " 'month': 0,\n",
              " 'dow': 0,\n",
              " 'area_name_en': 0,\n",
              " 'nearest_metro_en': 6837,\n",
              " 'nearest_mall_en': 6872,\n",
              " 'nearest_landmark_en': 1627,\n",
              " 'property_type_en': 0,\n",
              " 'rooms_en': 418}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "label_col = \"meter_sale_price\"\n",
        "\n",
        "numeric_cols = [\"procedure_area\", \"has_parking\", \"year\", \"month\", \"dow\"]\n",
        "cat_cols = [\"area_name_en\", \"nearest_metro_en\", \"nearest_mall_en\", \"nearest_landmark_en\",\n",
        "            \"property_type_en\", \"rooms_en\"]\n",
        "\n",
        "needed_cols = [label_col] + numeric_cols + cat_cols\n",
        "feature_cols = numeric_cols + cat_cols\n",
        "\n",
        "# Keep only needed columns\n",
        "df_reg = df2.select(needed_cols)\n",
        "\n",
        "\n",
        "df_reg = (\n",
        "    df_reg\n",
        "    .filter(F.col(label_col) > 0)\n",
        "    .filter(F.col(\"procedure_area\") > 0)\n",
        ")\n",
        "\n",
        "# Fill missing categoricals with \"unknown\"\n",
        "fill_map = {c: \"unknown\" for c in cat_cols}\n",
        "df_reg = df_reg.fillna(fill_map)\n",
        "\n",
        "# Fill missing has_parking with 0\n",
        "df_reg = df_reg.fillna({\"has_parking\": 0})\n",
        "\n",
        "print(\"Rows before:\", df2.count())\n",
        "print(\"Rows after :\", df_reg.count())\n",
        "\n",
        "df_reg.select(label_col, \"procedure_area\", \"area_name_en\", \"nearest_metro_en\", \"nearest_mall_en\").show(5, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPaKZFJAbwXs",
        "outputId": "0e6382e4-1678-4879-cbdd-cad424248561"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows before: 30173\n",
            "Rows after : 30173\n",
            "+----------------+--------------+------------------+--------------------------+------------------+\n",
            "|meter_sale_price|procedure_area|area_name_en      |nearest_metro_en          |nearest_mall_en   |\n",
            "+----------------+--------------+------------------+--------------------------+------------------+\n",
            "|5711.51         |151.8         |Burj Khalifa      |Business Bay Metro Station|Dubai Mall        |\n",
            "|2653.16         |76.0          |Al Warsan First   |Rashidiya Metro Station   |City Centre Mirdif|\n",
            "|3142.93         |356.3         |Al Thanayah Fourth|Nakheel Metro Station     |Marina Mall       |\n",
            "|3653.13         |159.3         |Al Goze Fourth    |Noor Bank Metro Station   |Dubai Mall        |\n",
            "|1772.12         |531.29        |Al Thanayah Fourth|Nakheel Metro Station     |Marina Mall       |\n",
            "+----------------+--------------+------------------+--------------------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "distinct_counts = df_reg.agg(\n",
        "    *[F.countDistinct(c).alias(c) for c in cat_cols]\n",
        ").collect()[0].asDict()\n",
        "\n",
        "distinct_counts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srts9x9KQ4ii",
        "outputId": "693b753f-4bc1-40e1-b781-106871523853"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'area_name_en': 67,\n",
              " 'nearest_metro_en': 38,\n",
              " 'nearest_mall_en': 6,\n",
              " 'nearest_landmark_en': 14,\n",
              " 'property_type_en': 2,\n",
              " 'rooms_en': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_reg.select([label_col] + feature_cols).describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07c1jMw0bwUQ",
        "outputId": "62499852-1ec1-4957-b76f-e626938b349a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+---------------+-----------------+------------------+--------------------+----------------+--------+\n",
            "|summary| meter_sale_price|    procedure_area|        has_parking|              year|             month|               dow|   area_name_en| nearest_metro_en|   nearest_mall_en| nearest_landmark_en|property_type_en|rooms_en|\n",
            "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+---------------+-----------------+------------------+--------------------+----------------+--------+\n",
            "|  count|            30173|             30173|              30173|             30173|             30173|             30173|          30173|            30173|             30173|               30173|           30173|   30173|\n",
            "|   mean|23615.96894077486|146.69104928247134| 0.7473237662811123|2012.9983428893381|6.6813376197262455|3.2582772677559406|           NULL|             NULL|              NULL|                NULL|            NULL|    NULL|\n",
            "| stddev|312756.4644882629|247.93351448611165|0.43455403935035486| 4.733693995963859| 3.310678877698124|1.2136842557966734|           NULL|             NULL|              NULL|                NULL|            NULL|    NULL|\n",
            "|    min|             0.01|              0.09|                  0|              2007|                 1|                 2|Al Barsha First|Airport Free Zone|City Centre Mirdif|Al Makhtoum Inter...|            Unit|   1 B/R|\n",
            "|    max|    1.109415556E7|           35703.0|                  1|              2026|                12|                 6| Zaabeel Second|          unknown|           unknown|             unknown|           Villa| unknown|\n",
            "+-------+-----------------+------------------+-------------------+------------------+------------------+------------------+---------------+-----------------+------------------+--------------------+----------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "AM8Q_3yHdCyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "\n",
        "\n",
        "label_col = \"meter_sale_price\"\n",
        "\n",
        "numeric_cols = [\"procedure_area\", \"has_parking\", \"year\", \"month\", \"dow\"]\n",
        "cat_cols = [\"area_name_en\", \"nearest_metro_en\", \"nearest_mall_en\",\n",
        "            \"nearest_landmark_en\", \"property_type_en\", \"rooms_en\"]\n",
        "\n",
        "# Index categorical columns\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\")\n",
        "    for c in cat_cols\n",
        "]\n",
        "\n",
        "# OneHot encode them\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=[f\"{c}_idx\" for c in cat_cols],\n",
        "    outputCols=[f\"{c}_ohe\" for c in cat_cols],\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "# Assemble everything into feature vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=numeric_cols + [f\"{c}_ohe\" for c in cat_cols],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler(\n",
        "    inputCol=\"features_raw\",\n",
        "    outputCol=\"features\",\n",
        "    withMean=True,\n",
        "    withStd=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7-5mfk3bwR6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Numeric columns:\", numeric_cols)\n",
        "print(\"Categorical columns:\", cat_cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg_7vmwCR0Wn",
        "outputId": "677f3af0-7c3b-4d7e-e05a-61c08d3e0ee5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric columns: ['procedure_area', 'has_parking', 'year', 'month', 'dow']\n",
            "Categorical columns: ['area_name_en', 'nearest_metro_en', 'nearest_mall_en', 'nearest_landmark_en', 'property_type_en', 'rooms_en']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = df_reg.randomSplit([0.8, 0.2], seed=42)\n",
        "print(\"Train:\", train_df.count(), \"Test:\", test_df.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LATfRbwdQC3",
        "outputId": "d7d99e44-fc17-4d3e-86f2-a51d19cc085f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 24263 Test: 5910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 — Linear Regression (Baseline)"
      ],
      "metadata": {
        "id": "vBFpvH1jd3Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=label_col)\n",
        "\n",
        "pipe_lr = Pipeline(stages=indexers + [encoder, assembler, scaler, lr])\n",
        "model_lr = pipe_lr.fit(train_df)\n",
        "\n",
        "pred_lr = model_lr.transform(test_df)\n",
        "\n",
        "rmse_eval = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "mae_eval  = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"mae\")\n",
        "r2_eval   = RegressionEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "\n",
        "rmse = rmse_eval.evaluate(pred_lr)\n",
        "mae  = mae_eval.evaluate(pred_lr)\n",
        "r2   = r2_eval.evaluate(pred_lr)\n",
        "\n",
        "print(f\"Linear Regression -> RMSE={rmse:.2f} | MAE={mae:.2f} | R2={r2:.4f}\")\n",
        "\n",
        "pred_lr.select(label_col, \"prediction\", \"procedure_area\", \"area_name_en\", \"nearest_metro_en\").show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JRAvTgDdP_a",
        "outputId": "7745363f-6562-485d-b8a7-12b341429852"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression -> RMSE=312022.94 | MAE=39021.79 | R2=0.0718\n",
            "+----------------+-------------------+--------------+------------------+---------------------+\n",
            "|meter_sale_price|prediction         |procedure_area|area_name_en      |nearest_metro_en     |\n",
            "+----------------+-------------------+--------------+------------------+---------------------+\n",
            "|269.87          |7426.639403459172  |100.17        |Wadi Al Safa 5    |unknown              |\n",
            "|504.98          |14747.630819908192 |1221.39       |Al Thanayah Fourth|Nakheel Metro Station|\n",
            "|542.9           |20913.540456543837 |745.11        |Wadi Al Safa 6    |unknown              |\n",
            "|705.64          |-418.6678913261203 |510.02        |Al Thanayah Fourth|Nakheel Metro Station|\n",
            "|826.4           |-12711.041560363552|291.56        |Al Thanayah Fourth|Damac Properties     |\n",
            "+----------------+-------------------+--------------+------------------+---------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 — Decision Tree Regression"
      ],
      "metadata": {
        "id": "-U_mzeXYeLi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "dt = DecisionTreeRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=label_col,\n",
        "    maxDepth=8,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pipe_dt = Pipeline(stages=indexers + [encoder, assembler, scaler, dt])\n",
        "model_dt = pipe_dt.fit(train_df)\n",
        "\n",
        "pred_dt = model_dt.transform(test_df)\n",
        "\n",
        "rmse_dt = rmse_eval.evaluate(pred_dt)\n",
        "mae_dt  = mae_eval.evaluate(pred_dt)\n",
        "r2_dt   = r2_eval.evaluate(pred_dt)\n",
        "\n",
        "print(f\"Decision Tree -> RMSE={rmse_dt:.2f} | MAE={mae_dt:.2f} | R2={r2_dt:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54si6qYVd2tM",
        "outputId": "d7846442-d797-48bf-94be-30e9434574f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree -> RMSE=43988.91 | MAE=4250.37 | R2=0.9816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model validation analysis"
      ],
      "metadata": {
        "id": "3qkiGSkwYvvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col_name in [\"procedure_area\", \"has_parking\", \"year\", \"month\", \"dow\"]:\n",
        "    corr = df_reg.stat.corr(\"meter_sale_price\", col_name)\n",
        "    print(col_name, \"correlation:\", corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWaexPKeUpYq",
        "outputId": "0056e045-cd04-4272-e8ef-8f47f8c40ffa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "procedure_area correlation: -0.029957118001162505\n",
            "has_parking correlation: 0.03226628907222292\n",
            "year correlation: -0.0078085660188522196\n",
            "month correlation: 0.003540467002043718\n",
            "dow correlation: 0.04151403601482724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Linear relationships are weak thats why LR underperformed\")"
      ],
      "metadata": {
        "id": "DdiDctpWZbrg",
        "outputId": "5456f54a-cc2c-43c4-eee2-c75d06e004c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear relationships are weak thats why LR underperformed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train R2:\")\n",
        "pred_train_dt = model_dt.transform(train_df)\n",
        "print(r2_eval.evaluate(pred_train_dt))\n",
        "\n",
        "print(\"Test R2:\")\n",
        "print(r2_dt)"
      ],
      "metadata": {
        "id": "q_2xaET2WYRx",
        "outputId": "9eef836d-e9fb-4c82-850a-bb0aa35ec360",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train R2:\n",
            "0.9653089127155318\n",
            "Test R2:\n",
            "0.9815524317738921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Results show that there is no major overfitting and the tree has generalized well\")"
      ],
      "metadata": {
        "id": "i6bUxs_UZpk0",
        "outputId": "5fbd779b-ed9d-4336-ba34-fc7519200606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results show that there is no major overfitting and the tree has generalized well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3 — RandomForest"
      ],
      "metadata": {
        "id": "B8QphtIsXB0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=label_col,\n",
        "    numTrees=80,\n",
        "    maxDepth=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pipe_rf = Pipeline(stages=indexers + [encoder, assembler, scaler, rf])\n",
        "model_rf = pipe_rf.fit(train_df)\n",
        "\n",
        "pred_rf = model_rf.transform(test_df)\n",
        "\n",
        "rmse_rf = rmse_eval.evaluate(pred_rf)\n",
        "mae_rf  = mae_eval.evaluate(pred_rf)\n",
        "r2_rf   = r2_eval.evaluate(pred_rf)\n",
        "\n",
        "print(f\"Random Forest -> RMSE={rmse_rf:.2f} | MAE={mae_rf:.2f} | R2={r2_rf:.4f}\")"
      ],
      "metadata": {
        "id": "TaxyySe-XTLe",
        "outputId": "1d50feb3-84d6-4746-bb0e-a2e9ebce4579",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest -> RMSE=60903.11 | MAE=5290.98 | R2=0.9646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lr.select(label_col, \"prediction\").show(5)\n",
        "pred_dt.select(label_col, \"prediction\").show(5)\n",
        "pred_rf.select(label_col, \"prediction\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbtD_iX0d2ps",
        "outputId": "896f3e1c-0ac2-4bea-8046-5de36e9ccd16"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------------+\n",
            "|meter_sale_price|         prediction|\n",
            "+----------------+-------------------+\n",
            "|          269.87|  7426.639403459172|\n",
            "|          504.98| 14747.630819908192|\n",
            "|           542.9| 20913.540456543837|\n",
            "|          705.64| -418.6678913261203|\n",
            "|           826.4|-12711.041560363552|\n",
            "+----------------+-------------------+\n",
            "only showing top 5 rows\n",
            "+----------------+------------------+\n",
            "|meter_sale_price|        prediction|\n",
            "+----------------+------------------+\n",
            "|          269.87| 7791.114618443687|\n",
            "|          504.98|1248.0702727272726|\n",
            "|           542.9| 3343.613353293414|\n",
            "|          705.64|1248.0702727272726|\n",
            "|           826.4| 5887.941458167331|\n",
            "+----------------+------------------+\n",
            "only showing top 5 rows\n",
            "+----------------+------------------+\n",
            "|meter_sale_price|        prediction|\n",
            "+----------------+------------------+\n",
            "|          269.87| 7308.485728140845|\n",
            "|          504.98|1441.1824985522994|\n",
            "|           542.9|2776.8805277700585|\n",
            "|          705.64|1492.6450700334308|\n",
            "|           826.4|4092.8711463068285|\n",
            "+----------------+------------------+\n",
            "only showing top 5 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual CV - Decision Tree\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qAuMmINZbXO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, floor\n",
        "\n",
        "k = 10\n",
        "\n",
        "df_cv = train_df.withColumn(\"fold\", floor(rand(seed=42) * k))\n",
        "\n",
        "df_cv.groupBy(\"fold\").count().show()"
      ],
      "metadata": {
        "id": "CNQoz4Pfbr7R",
        "outputId": "7e69c31a-de75-440d-a9ee-acfda55b9329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+\n",
            "|fold|count|\n",
            "+----+-----+\n",
            "|   0| 2391|\n",
            "|   7| 2440|\n",
            "|   6| 2419|\n",
            "|   9| 2343|\n",
            "|   5| 2481|\n",
            "|   1| 2481|\n",
            "|   3| 2415|\n",
            "|   8| 2368|\n",
            "|   2| 2395|\n",
            "|   4| 2530|\n",
            "+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "dt = DecisionTreeRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=label_col,\n",
        "    maxDepth=8,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "rmse_scores = []\n",
        "\n",
        "for fold in range(k):\n",
        "\n",
        "    train_fold = df_cv.filter(df_cv.fold != fold)\n",
        "    val_fold   = df_cv.filter(df_cv.fold == fold)\n",
        "\n",
        "    pipe = Pipeline(stages=indexers + [encoder, assembler, scaler, dt])\n",
        "    model = pipe.fit(train_fold)\n",
        "\n",
        "    preds = model.transform(val_fold)\n",
        "\n",
        "    rmse = rmse_eval.evaluate(preds)\n",
        "    rmse_scores.append(rmse)\n",
        "\n",
        "    print(f\"Fold {fold} RMSE: {rmse:.2f}\")\n",
        "\n",
        "print(\"Average RMSE:\", sum(rmse_scores)/len(rmse_scores))"
      ],
      "metadata": {
        "id": "Mjm4Ak4_cTZG",
        "outputId": "1d1c9379-2cfe-4c19-dabb-869c9b59a2a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0 RMSE: 18633.36\n",
            "Fold 1 RMSE: 108073.11\n",
            "Fold 2 RMSE: 82140.80\n",
            "Fold 3 RMSE: 31562.31\n",
            "Fold 4 RMSE: 41666.85\n",
            "Fold 5 RMSE: 144988.07\n",
            "Fold 6 RMSE: 157469.06\n",
            "Fold 7 RMSE: 62500.64\n",
            "Fold 8 RMSE: 67545.96\n",
            "Fold 9 RMSE: 44690.91\n",
            "Average RMSE: 75927.10663991304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual CV - Linear Regression\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "08jVU1DtfvYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=label_col)\n",
        "\n",
        "rmse_lr_scores = []\n",
        "\n",
        "for fold in range(k):\n",
        "\n",
        "    train_fold = df_cv.filter(df_cv.fold != fold)\n",
        "    val_fold   = df_cv.filter(df_cv.fold == fold)\n",
        "\n",
        "    pipe = Pipeline(stages=indexers + [encoder, assembler, scaler, lr])\n",
        "    model = pipe.fit(train_fold)\n",
        "\n",
        "    preds = model.transform(val_fold)\n",
        "\n",
        "    rmse = rmse_eval.evaluate(preds)\n",
        "    rmse_lr_scores.append(rmse)\n",
        "\n",
        "    print(f\"LR Fold {fold} RMSE: {rmse:.2f}\")\n",
        "\n",
        "print(\"LR Average RMSE:\", sum(rmse_lr_scores)/len(rmse_lr_scores))"
      ],
      "metadata": {
        "id": "igazAZsNf11V",
        "outputId": "6b71d010-3b34-4fdd-ce59-5c7110d47bfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR Fold 0 RMSE: 273007.23\n",
            "LR Fold 1 RMSE: 342460.01\n",
            "LR Fold 2 RMSE: 340887.24\n",
            "LR Fold 3 RMSE: 272975.02\n",
            "LR Fold 4 RMSE: 247539.88\n",
            "LR Fold 5 RMSE: 313628.03\n",
            "LR Fold 6 RMSE: 428906.59\n",
            "LR Fold 7 RMSE: 217027.85\n",
            "LR Fold 8 RMSE: 274601.32\n",
            "LR Fold 9 RMSE: 246549.54\n",
            "LR Average RMSE: 295758.2704787035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual CV - Random Forest\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "68intDEZjR94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=label_col,\n",
        "    numTrees=80,\n",
        "    maxDepth=10,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "rmse_rf_scores = []\n",
        "\n",
        "for fold in range(k):\n",
        "\n",
        "    train_fold = df_cv.filter(df_cv.fold != fold)\n",
        "    val_fold   = df_cv.filter(df_cv.fold == fold)\n",
        "\n",
        "    pipe = Pipeline(stages=indexers + [encoder, assembler, scaler, rf])\n",
        "    model = pipe.fit(train_fold)\n",
        "\n",
        "    preds = model.transform(val_fold)\n",
        "\n",
        "    rmse = rmse_eval.evaluate(preds)\n",
        "    rmse_rf_scores.append(rmse)\n",
        "\n",
        "    print(f\"RF Fold {fold} RMSE: {rmse:.2f}\")\n",
        "\n",
        "print(\"RF Average RMSE:\", sum(rmse_rf_scores)/len(rmse_rf_scores))"
      ],
      "metadata": {
        "id": "Px-kcyLAjZnD",
        "outputId": "06c32d05-f843-4ffc-9dbf-95760f17f911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Fold 0 RMSE: 32573.59\n",
            "RF Fold 1 RMSE: 52809.36\n",
            "RF Fold 2 RMSE: 89267.25\n",
            "RF Fold 3 RMSE: 35494.23\n",
            "RF Fold 4 RMSE: 47731.92\n",
            "RF Fold 5 RMSE: 128219.94\n",
            "RF Fold 6 RMSE: 123140.47\n",
            "RF Fold 7 RMSE: 54043.65\n",
            "RF Fold 8 RMSE: 75034.83\n",
            "RF Fold 9 RMSE: 48008.39\n",
            "RF Average RMSE: 68632.36410951514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble Model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Hse4ujsyniD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "train_df = train_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "test_df  = test_df.withColumn(\"row_id\", monotonically_increasing_id())"
      ],
      "metadata": {
        "id": "Hw1qeIq_sr6d"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "B = 10\n",
        "\n",
        "dt_base = DecisionTreeRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=label_col,\n",
        "    maxDepth=8,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pred_list = []\n",
        "\n",
        "for b in range(B):\n",
        "    boot_train = train_df.sample(withReplacement=True, fraction=1.0, seed=100 + b)\n",
        "\n",
        "    pipe = Pipeline(stages=indexers + [encoder, assembler, scaler, dt_base])\n",
        "    model = pipe.fit(boot_train)\n",
        "\n",
        "    preds = model.transform(test_df).select(\n",
        "        \"row_id\",\n",
        "        F.col(\"prediction\").alias(f\"pred_{b}\")\n",
        "    )\n",
        "\n",
        "    pred_list.append(preds)\n",
        "    print(f\"Bag model {b+1}/{B} done\")"
      ],
      "metadata": {
        "id": "YprnXtazn5KM",
        "outputId": "d36d1410-2a83-431d-bd76-37e9bdbd82b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag model 1/10 done\n",
            "Bag model 2/10 done\n",
            "Bag model 3/10 done\n",
            "Bag model 4/10 done\n",
            "Bag model 5/10 done\n",
            "Bag model 6/10 done\n",
            "Bag model 7/10 done\n",
            "Bag model 8/10 done\n",
            "Bag model 9/10 done\n",
            "Bag model 10/10 done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join all prediction columns by row_id\n",
        "bagged = pred_list[0]\n",
        "for i in range(1, B):\n",
        "    bagged = bagged.join(pred_list[i], on=\"row_id\", how=\"inner\")\n",
        "\n",
        "# bring true label from test_df\n",
        "bagged = bagged.join(test_df.select(\"row_id\", label_col), on=\"row_id\", how=\"inner\")\n",
        "\n",
        "# average predictions\n",
        "pred_cols = [F.col(f\"pred_{i}\") for i in range(B)]\n",
        "bagged = bagged.withColumn(\"prediction\", sum(pred_cols) / F.lit(B))\n",
        "\n",
        "# evaluate\n",
        "rmse_bag = rmse_eval.evaluate(bagged)\n",
        "mae_bag  = mae_eval.evaluate(bagged)\n",
        "r2_bag   = r2_eval.evaluate(bagged)\n",
        "\n",
        "print(f\"Bagging -> RMSE={rmse_bag:.2f} | MAE={mae_bag:.2f} | R2={r2_bag:.4f}\")"
      ],
      "metadata": {
        "id": "BPvErsr2rlLS",
        "outputId": "75ec4aaa-6fa5-490d-cc48-0864431d6e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging -> RMSE=44032.79 | MAE=4191.56 | R2=0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Regression significantly outperformed Linear Regression across all metrics and was therefore selected as the final regression model"
      ],
      "metadata": {
        "id": "JAq_TGC2im5Z"
      }
    }
  ]
}